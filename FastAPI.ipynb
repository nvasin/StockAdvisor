{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6U4NSQQSXEyd"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install \"fastapi[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNDV1uq0Yupi",
    "outputId": "0670abad-a049-4b0b-ed3a-bffdb77df67b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from http import HTTPStatus\n",
    "from typing import Dict, List, Union, Optional\n",
    "from enum import Enum\n",
    "\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "\n",
    "\n",
    "app = FastAPI(\n",
    "    docs_url=\"/api/openapi\",\n",
    "    openapi_url=\"/api/openapi.json\",\n",
    ")\n",
    "\n",
    "\n",
    "class Hyperparameters(BaseModel):\n",
    "    optimized: Optional[bool]\n",
    "\n",
    "class ModelTypeEnum(str, Enum):\n",
    "  ses = 'ses'\n",
    "  arima = 'arima'\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "  hyperparameters: Hyperparameters\n",
    "  id: str\n",
    "  ml_model_type: ModelTypeEnum\n",
    "\n",
    "class ForecastRequest(BaseModel):\n",
    "  X: List[List[float]]\n",
    "  config: ModelConfig\n",
    "  steps: int\n",
    "\n",
    "class ForecastResponse(BaseModel):\n",
    "  id: str\n",
    "  predictions: List[float]\n",
    "\n",
    "class LoadRequest(BaseModel):\n",
    "  id: str\n",
    "\n",
    "class LoadResponse(BaseModel):\n",
    "  message: str\n",
    "\n",
    "class ModelListResponse(BaseModel):\n",
    "  id: str\n",
    "\n",
    "class RemoveResponse(BaseModel):\n",
    "  message: str\n",
    "\n",
    "\n",
    "models = {}\n",
    "inf_models = {}\n",
    "\n",
    "def ses_forecast(train, steps):\n",
    "  preds = []\n",
    "  cur = train['Price'].copy()\n",
    "  idx = cur.index[-1]\n",
    "\n",
    "  for _ in range(steps):\n",
    "      model = SimpleExpSmoothing(cur).fit(optimized=True)\n",
    "      alpha = model.params['smoothing_level']  # Получаем значение alpha\n",
    "      print(f\"Параметр SES: alpha={alpha:.4f}\")\n",
    "\n",
    "      f = model.forecast(1).iloc[-1]\n",
    "\n",
    "      lo = f * 0.95\n",
    "      hi = f * 1.05\n",
    "      preds.append((f, lo, hi))\n",
    "\n",
    "      idx += pd.Timedelta(days=1)\n",
    "      cur.loc[idx] = f\n",
    "\n",
    "  return preds\n",
    "\n",
    "def arima_forecast(a,b):\n",
    "  pass\n",
    "\n",
    "\n",
    "@app.post(\"/forecast\", response_model=list[ForecastResponse], status_code=HTTPStatus.CREATED)\n",
    "async def fit(request:list[ForecastRequest]):\n",
    "  list_of_inputs = jsonable_encoder(request)\n",
    "  for input in list_of_inputs:\n",
    "    if input['config']['ml_model_type'] == 'ses':\n",
    "      preds = ses_forecast(input['X'], input['steps'])\n",
    "    else:\n",
    "      preds = arima_forecast(input['X'], input['steps'])\n",
    "  return [{'id': input['id'], 'predictions': preds}]\n",
    "\n",
    "\n",
    "@app.post(\"/load\", response_model=list[LoadResponse])\n",
    "async def load(request: LoadRequest):\n",
    "  input = jsonable_encoder(request)\n",
    "  global inf_models\n",
    "  inf_models[input['id']] = models[input['id']]\n",
    "  return [{'message': f\"Model '{input['id']}' loaded\"}]\n",
    "\n",
    "\n",
    "@app.get(\"/list_models\", response_model=list[ModelListResponse])\n",
    "async def list_models():\n",
    "  res = []\n",
    "  for k in models.keys():\n",
    "    res.append({'id': k})\n",
    "  return res\n",
    "\n",
    "\n",
    "@app.delete(\"/remove_all\", response_model=list[RemoveResponse])\n",
    "async def remove_all():\n",
    "  global models\n",
    "  res = []\n",
    "  for k in models.keys():\n",
    "    res.append({'message': f\"Model '{k}' removed\"})\n",
    "  models = []\n",
    "  return res\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awXoExkDXCxX",
    "outputId": "d99372c4-b985-421c-fd61-ab6bb1cb2bd4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "35.204.224.125"
     ]
    }
   ],
   "source": [
    "!curl https://loca.lt/mytunnelpassword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-yX5bBtjqso",
    "outputId": "be2acdcc-4752-408f-8959-024b9caafacf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[1G\u001B[0K⠙\u001B[1G\u001B[0K⠹\u001B[1G\u001B[0K⠸\u001B[1G\u001B[0K⠼\u001B[1G\u001B[0K⠴\u001B[1G\u001B[0K⠦\u001B[1G\u001B[0K⠧\u001B[1G\u001B[0K⠇\u001B[1G\u001B[0K⠏\u001B[1G\u001B[0K⠋\u001B[1G\u001B[0K⠙\u001B[1G\u001B[0K⠹\u001B[1G\u001B[0K⠸\u001B[1G\u001B[0K⠼\u001B[1G\u001B[0K⠴\u001B[1G\u001B[0K⠦\u001B[1G\u001B[0K⠧\u001B[1G\u001B[0K⠇\u001B[1G\u001B[0K⠏\u001B[1G\u001B[0K⠋\u001B[1G\u001B[0K⠙\u001B[1G\u001B[0K⠹\u001B[1G\u001B[0K⠸\u001B[1G\u001B[0K⠼\u001B[1G\u001B[0K⠴\u001B[1G\u001B[0K⠦\u001B[1G\u001B[0K⠧\u001B[1G\u001B[0K\n",
      "added 22 packages in 3s\n",
      "\u001B[1G\u001B[0K⠧\u001B[1G\u001B[0K\n",
      "\u001B[1G\u001B[0K⠧\u001B[1G\u001B[0K3 packages are looking for funding\n",
      "\u001B[1G\u001B[0K⠧\u001B[1G\u001B[0K  run `npm fund` for details\n",
      "\u001B[1G\u001B[0K⠧\u001B[1G\u001B[0K"
     ]
    }
   ],
   "source": [
    "!npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDNpWGkBXG29",
    "outputId": "de94b4e8-7ad6-4c67-d211-296d40cf237c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[1G\u001B[0K⠙\u001B[1G\u001B[0K⠹\u001B[1G\u001B[0K⠸\u001B[1G\u001B[0K⠼\u001B[1G\u001B[0K⠴\u001B[1G\u001B[0K⠦\u001B[1G\u001B[0K⠧\u001B[1G\u001B[0K⠇\u001B[1G\u001B[0K⠏\u001B[1G\u001B[0Kyour url is: https://fastapi.loca.lt\n",
      "\u001B[32mINFO\u001B[0m:     Started server process [\u001B[36m36844\u001B[0m]\n",
      "\u001B[32mINFO\u001B[0m:     Waiting for application startup.\n",
      "\u001B[32mINFO\u001B[0m:     Application startup complete.\n",
      "\u001B[32mINFO\u001B[0m:     Uvicorn running on \u001B[1mhttp://127.0.0.1:8000\u001B[0m (Press CTRL+C to quit)\n",
      "\u001B[32mINFO\u001B[0m:     85.140.5.120:0 - \"\u001B[1mGET / HTTP/1.1\u001B[0m\" \u001B[31m404 Not Found\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m:     85.140.5.120:0 - \"\u001B[1mGET /api/openapi HTTP/1.1\u001B[0m\" \u001B[32m200 OK\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m:     85.140.5.120:0 - \"\u001B[1mGET /api/openapi.json HTTP/1.1\u001B[0m\" \u001B[32m200 OK\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m:     85.140.5.120:0 - \"\u001B[1mPOST /forecast HTTP/1.1\u001B[0m\" \u001B[91m500 Internal Server Error\u001B[0m\n",
      "\u001B[31mERROR\u001B[0m:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/content/main.py\", line 105, in fit\n",
      "    preds = ses_forecast(input['X'], input['steps'])\n",
      "  File \"/content/main.py\", line 78, in ses_forecast\n",
      "    idx = cur.index[-1]\n",
      "TypeError: 'builtin_function_or_method' object is not subscriptable\n",
      "\u001B[32mINFO\u001B[0m:     Shutting down\n",
      "\u001B[32mINFO\u001B[0m:     Waiting for application shutdown.\n",
      "\u001B[32mINFO\u001B[0m:     Application shutdown complete.\n",
      "\u001B[32mINFO\u001B[0m:     Finished server process [\u001B[36m36844\u001B[0m]\n"
     ]
    }
   ],
   "source": [
    "!uvicorn main:app & npx localtunnel --port 8000 --subdomain fastapi & wget -q -O - https://loca.lt/mytunnelpassword"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
